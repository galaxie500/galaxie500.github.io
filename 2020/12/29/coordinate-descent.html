<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Solving a coordinate descent in 3 ways | brianHu</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Solving a coordinate descent in 3 ways" />
<meta name="author" content="Brian Hu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="What is a coordinate descent compared to gradient descent? How to solve it?" />
<meta property="og:description" content="What is a coordinate descent compared to gradient descent? How to solve it?" />
<link rel="canonical" href="https://galaxie500.github.io/2020/12/29/coordinate-descent.html" />
<meta property="og:url" content="https://galaxie500.github.io/2020/12/29/coordinate-descent.html" />
<meta property="og:site_name" content="brianHu" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Solving a coordinate descent in 3 ways" />
<script type="application/ld+json">
{"description":"What is a coordinate descent compared to gradient descent? How to solve it?","@type":"BlogPosting","headline":"Solving a coordinate descent in 3 ways","dateModified":"2020-12-29T00:00:00+00:00","datePublished":"2020-12-29T00:00:00+00:00","url":"https://galaxie500.github.io/2020/12/29/coordinate-descent.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://galaxie500.github.io/2020/12/29/coordinate-descent.html"},"author":{"@type":"Person","name":"Brian Hu"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css">
  <link rel="icon" type="image/png" href="/assets/favicon.png" />
  <link rel="stylesheet" href="/assets/css/magnific-popup.css"><link type="application/atom+xml" rel="alternate" href="https://galaxie500.github.io/feed.xml" title="brianHu" /><script src="https://code.jquery.com/jquery-3.2.0.min.js"></script> 
  <script src="/assets/js/jquery.magnific-popup.js"></script>
  <!--mathJax-->
  
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
</head>
<body><div class="site-header">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">brianHu<b class="command_prompt"></b><b class="blinking_cursor">_</b></a>
    <span class="social_links">
        
        
        <a class="color-cyan-hover" href="https://twitter.com/bitbrain_"><i class="fab fa-twitter-square"></i></a>
        
        
        
        <a class="color-purple-hover" href="https://www.twitch.tv/bitbrain_"><i class="fab fa-twitch"></i></a>
        
        
        
        <a class="color-red-hover" href="https://www.youtube.com/channel/UCZDjQltHRNiXIYXMBeLDleA"><i class="fab fa-youtube"></i></a>
        
        
        
        <a class="color-purple-hover" href="https://github.com/galaxie500"><i class="fab fa-github-square"></i></a>
        
        
    </span>
  </div>
</div>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        
  <div class="author-box">
    
    
        <img src="
            https://gravatar.com/avatar/014519ba564bb4fe618246348fbda119?s=256
        " class="author-avatar" alt="Avatar" />
    
Current Master of Data Science Candidate @ Univ of California San Diego, Data Engineer Intern @ Gotion, I blog about stuff I learned form data cience, data engineer and big data analytics.

</div>


<div class="post">
  <h1 class="post-title">Solving a coordinate descent in 3 ways</h1>
  
  <div class="post-tags">
      
      <a class="tag" href="/tag/ml/">ml</a>
      
      <a class="tag" href="/tag/introduction/">introduction</a>
      
  </div>
  
  <div class="post-date">Published on 29 Dec 2020</div>
  
  <noscript>
    <div class="post-description">What is a coordinate descent compared to gradient descent? How to solve it?</div>
  </noscript>
  <div id="animated-post-description" class="post-description" style="display: none;"></div>
  
  <h1 id="introduction">Introduction</h1>

<p>In this article, we consider a standard unconstrained optimization problem on solving logistic regression with 
<a href="https://github.com/galaxie500/dataScienceMiniProjects/blob/main/Coordinate_Descent/heart.csv">heart disease data set</a>:</p>

\[min L(w)\]

<p>where \(L(·)\) is some cost function and \(w \in \mathbb{R}^d\). In class, we looked at several approaches to solving such
problems—such as gradient descent and stochastic gradient descent—under differentiability conditions on
\(L(w)\). We will now look at a different, and in many ways simpler, approach:</p>
<ol>
  <li>Initialize \(w\) somehow.</li>
  <li>Repeat: pick a coordinate \(i \in \{1, 2, ... , d\}\), and update the value of \(w_i\) so as to reduce the loss.</li>
</ol>

<p>Two questions need to be answered in order to fully specify the updates:</p>

<ol>
  <li>Which coordinate to choose?</li>
  <li>How to set the new value of \(w_i\)?</li>
</ol>

<h1 id="description-of-coordinate-descent-method">Description of coordinate descent method</h1>

<h3 id="key-strategies">Key Strategies:</h3>
<ul>
  <li>Objective function(Loss function) and gradients:</li>
</ul>

<p>The Maximum likelihood estimation of logistic regression is in a form of:</p>

\[L(w) = -1/n\displaystyle\sum_{i=1}^{n} y_{i} \log \hat{y}_{i} + (1 - y_{i}) \log(1 - \hat{y}_{i})\]

<p>We transform \(\hat y = w^T x_i +b\) by a Sigmoid function \(\sigma(x)=\frac{1}{1+e^{-x}}\),
hence, the loss function can be writen as:</p>

\[L(w) = -1/n\displaystyle\sum_{i=1}^{n} y_{i} \log \sigma(\hat{y}_{i}) + (1 - y_{i}) \log(1 - \sigma(\hat{y}_{i}))\]

<p>Thus, the coordinate gradient with respect with each weight \(w_{j}\) is given by</p>

\[\frac{\partial L(w)}{\partial w_{j}} = \displaystyle \sum_{i=1}^{n}(\sigma(\hat{y}_{i}) - y_{i})x_{j}\]

<ul>
  <li>Rules for selecting coordinates:</li>
</ul>

<ol>
  <li>randomly pick a coordinate \(i\) at a time</li>
  <li><strong>Cyclic coordinate descent</strong>: go through all the features cyclically. (1,2,..13,1,2,…13,1,2…until converge.)</li>
  <li><strong>Gauss-Southwell Method</strong>: at each iteration, pick coordinate \(i\) that has greatest gradient in absolute value for which:</li>
</ol>

\[i = \underset{1 \leq j \leq n}{\operatorname{argmax}|\nabla_j f(x^{(t)})|}\]

\[\nabla_j f(x^{(t)}) = \frac{\partial L(w)}{\partial w_{j}} = \displaystyle \sum_{i=1}^{n}(\sigma(\hat{y}_{i}) - y_{i})x_{j}\]

<ul>
  <li>Rules for updating \(\omega_i\):</li>
</ul>

\[w_{i} = w_{i} - \eta \frac{\partial L(w)}{\partial w_{i}}\]

<p>Here, \(\eta\) denotes step size.</p>

<ul>
  <li>Constraint on loss function: <strong>L(·) should be convex and differentiable</strong>.</li>
</ul>

<h1 id="convergence">Convergence</h1>

<p>Under what conditions do you think our method converges to the optimal loss?</p>

<p>The objective function(Loss function) values are non-decreasing: \(f(x(0)) \geq f(x(1)) \geq f(x(2)) ...\), if \(f\) is strictly convex and differentiable, the algorithm converges to a global minimum. Hence, we we let iterations keep going until the loss gets as close to as \(L^*\), which is the fitting Loss of our baseline logistic regrssion model.</p>

<p>We will take the difference between fitting loss and \(L^*\), if the difference is less than \(0.001\), we will consider the fitting process converged to a gloabal minimum, then the iterations will terminate.</p>

<h1 id="experiment-results">Experiment Results</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">log_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</code></pre></div></div>

<ul>
  <li>Prepare data</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s">'heart.csv'</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">','</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># normalize features
</span><span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="c1"># add intercept-1 vector to feature matrix
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))))</span>
<span class="c1"># examine data
</span><span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)).</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(303, 14)
(303, 1)
</code></pre></div></div>

<ul>
  <li>Baseline Loss on Logistic Regression</li>
</ul>

<p>Begin by running a standard logistic regression solver(not regularized) on the training set. Make note of the final loss \(L^*\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># baseline logistic regression model without regularization
</span><span class="n">reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s">'sag'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">L_base</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Baseline(Logistic Regression) Training Loss L* = </span><span class="si">{</span><span class="n">L_base</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Baseline(Logistic Regression) Training Loss L* = 0.3489042453037286
</code></pre></div></div>

<p>Then, implement your coordinate descent method and run it on this data.</p>

<ul>
  <li>Implementing CoordinateDescent Classifier.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CoordinateDescent</span><span class="p">():</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">select</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>    
        <span class="s">"""Create a Coordinate Descent Classifier.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">step</span> <span class="o">=</span> <span class="n">step</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">select</span> <span class="o">=</span> <span class="n">select</span>
     
    
    <span class="k">def</span> <span class="nf">__sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""Squashing function.
        """</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
        
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Update coefficient based on different strategy of selecting one coordinate each time.
        Fitting process will be ended while converging criteria met.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">L</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="n">diff</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">cycle_digit</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">while</span> <span class="n">diff</span> <span class="o">&gt;=</span> <span class="mf">0.001</span><span class="p">:</span> 
            <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">__sigmoid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">L</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">),</span> <span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># gradients
</span>
            <span class="n">cycle_index</span> <span class="o">=</span> <span class="n">cycle_digit</span> <span class="o">%</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">select</span> <span class="o">==</span> <span class="s">'cgd'</span><span class="p">:</span> <span class="c1"># select with largest gradient in absolute value
</span>                <span class="n">to_update</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">delta</span><span class="p">))</span>
                <span class="n">w</span><span class="p">[</span><span class="n">to_update</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">to_update</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">step</span> <span class="o">*</span> <span class="n">delta</span><span class="p">[</span><span class="n">to_update</span><span class="p">]</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">select</span> <span class="o">==</span> <span class="s">'rand'</span><span class="p">:</span> <span class="c1"># randomly select coordinate
</span>                <span class="n">random</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">)</span>
                <span class="n">w</span><span class="p">[</span><span class="n">random</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">random</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">step</span> <span class="o">*</span> <span class="n">delta</span><span class="p">[</span><span class="n">random</span><span class="p">]</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">select</span> <span class="o">==</span> <span class="s">'cycle'</span><span class="p">:</span> <span class="c1"># cyclic go through all coordinate
</span>                <span class="n">w</span><span class="p">[</span><span class="n">cycle_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">cycle_index</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">step</span> <span class="o">*</span> <span class="n">delta</span><span class="p">[</span><span class="n">cycle_index</span><span class="p">]</span>
                <span class="n">cycle_digit</span> <span class="o">+=</span> <span class="mi">1</span> 
            
            <span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">target</span>
            
            <span class="bp">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">w</span>
            
        <span class="k">return</span> <span class="bp">self</span>
   

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">"""Calculate OLS result with coefficients trained by fittting.
        """</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">__sigmoid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">.</span><span class="n">flatten</span><span class="p">())))</span>
        <span class="k">return</span> <span class="n">y_pred</span>
   

    <span class="k">def</span> <span class="nf">get_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">L</span>
    
    
    <span class="k">def</span> <span class="nf">get_coef</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">w</span>
    
    
    <span class="k">def</span> <span class="nf">get_num_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span>
</code></pre></div></div>

<ul>
  <li>Method 1: Cyclic go through each coordinate \(i\)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fit training data with coordinate descent model(default 1050 iterations)
</span><span class="n">m1</span> <span class="o">=</span> <span class="n">CoordinateDescent</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">L_base</span><span class="p">,</span> <span class="n">select</span><span class="o">=</span><span class="s">'cycle'</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">m1_pred</span> <span class="o">=</span> <span class="n">m1</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Accuracy
</span><span class="n">m1_acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m1_pred</span><span class="p">)</span>
<span class="c1"># Training Loss
</span><span class="n">L1</span> <span class="o">=</span> <span class="n">m1</span><span class="p">.</span><span class="n">get_loss</span><span class="p">()</span>
<span class="n">num_iter1</span> <span class="o">=</span> <span class="n">m1</span><span class="p">.</span><span class="n">get_num_iter</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Go through each coordinate cyclically: </span><span class="se">\n\
</span><span class="s">-------------------------------------- </span><span class="se">\
\n</span><span class="s">Accuracy:       -&gt; </span><span class="si">{</span><span class="n">m1_acc</span><span class="si">}</span><span class="s"> </span><span class="se">\
\n</span><span class="s">Fitting Loss:   -&gt; </span><span class="si">{</span><span class="n">L1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\
\n</span><span class="s">#Iterations:    -&gt; </span><span class="si">{</span><span class="n">num_iter1</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Go through each coordinate cyclically: 
-------------------------------------- 
Accuracy:       -&gt; 0.8514851485148515 
Fitting Loss:   -&gt; 0.349902342590921 
#Iterations:    -&gt; 3576
</code></pre></div></div>

<ul>
  <li>Method 2: Pick coordinate $i$ that has greatest gradient in absolute value at each iteration</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fit training data with coordinate descent model
</span><span class="n">m2</span> <span class="o">=</span> <span class="n">CoordinateDescent</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">L_base</span><span class="p">,</span> <span class="n">select</span><span class="o">=</span><span class="s">'cgd'</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">m2_pred</span> <span class="o">=</span> <span class="n">m2</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Accuracy
</span><span class="n">m2_acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m2_pred</span><span class="p">)</span>
<span class="c1"># Training Loss
</span><span class="n">L2</span> <span class="o">=</span> <span class="n">m2</span><span class="p">.</span><span class="n">get_loss</span><span class="p">()</span>
<span class="n">num_iter2</span> <span class="o">=</span> <span class="n">m2</span><span class="p">.</span><span class="n">get_num_iter</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Select feature based on largest gradient: </span><span class="se">\n\
</span><span class="s">----------------------------------------- </span><span class="se">\
\n</span><span class="s">Accuracy:       -&gt; </span><span class="si">{</span><span class="n">m2_acc</span><span class="si">}</span><span class="s"> </span><span class="se">\
\n</span><span class="s">Fitting Loss:   -&gt; </span><span class="si">{</span><span class="n">L2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\
\n</span><span class="s">#Iterations:    -&gt; </span><span class="si">{</span><span class="n">num_iter2</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Select feature based on largest gradient: 
----------------------------------------- 
Accuracy:       -&gt; 0.8382838283828383 
Fitting Loss:   -&gt; 0.3499040333347324 
#Iterations:    -&gt; 1622
</code></pre></div></div>

<ul>
  <li>Finally, compare to a method that chooses coordinates \(i\) uniformly at random, we’ll call this “random-feature coordinate descent”.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m3</span> <span class="o">=</span> <span class="n">CoordinateDescent</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">L_base</span><span class="p">,</span> <span class="n">select</span><span class="o">=</span><span class="s">'rand'</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">m3_pred</span> <span class="o">=</span> <span class="n">m3</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Accuracy
</span><span class="n">m3_acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">m3_pred</span><span class="p">)</span>
<span class="c1"># Training Loss
</span><span class="n">L3</span> <span class="o">=</span> <span class="n">m3</span><span class="p">.</span><span class="n">get_loss</span><span class="p">()</span>
<span class="n">num_iter3</span> <span class="o">=</span> <span class="n">m3</span><span class="p">.</span><span class="n">get_num_iter</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Randomly select coordinate: </span><span class="se">\n\
</span><span class="s">---------------------------------- </span><span class="se">\
\n</span><span class="s">Accuracy:       -&gt; </span><span class="si">{</span><span class="n">m3_acc</span><span class="si">}</span><span class="s"> </span><span class="se">\
\n</span><span class="s">Fitting Loss:   -&gt; </span><span class="si">{</span><span class="n">L3</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\
\n</span><span class="s">#Iterations:    -&gt; </span><span class="si">{</span><span class="n">num_iter3</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<p>Output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Randomly select coordinate: 
---------------------------------- 
Accuracy:       -&gt; 0.8514851485148515 
Fitting Loss:   -&gt; 0.3499041459249309 
#Iterations:    -&gt; 4893
</code></pre></div></div>

<h1 id="conclusion">Conclusion</h1>

<p>Three different methods all converged to \(L^*\) within a sufficiently small range, however, they do take different number of iterations to achieve their results, apparently selecting coordinate based on the largest coordinate gradient requires the smallest number of iterations(\(1622\)), which indicating this method converging faster than the others, respectively, cyclic coordinate gradient takes \(3576\) iterations, random selecting takes \(4800-5500\) times, for random selecting, even though the number of iterations may differ each time, the difference is trivial.</p>

<p>We visualize this result by plotting the fitting curves below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_loss</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">L1</span><span class="p">,</span> <span class="s">'green'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'"Cyclic Coordinate Descent"'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">L2</span><span class="p">,</span> <span class="s">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'"Coordinate Gradient Descent"'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">L3</span><span class="p">,</span> <span class="s">'orange'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'"Random Coordinate Descent"'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">L_base</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightblue'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Baseline L*'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Coordinate Descent Fitting Loss'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Iterations'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Fitting Loss'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_loss</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/coordinate_descent.png" /></p>

<p>The graph above shows how the loss of our “adaptive” coordinate descent algorithm’s current iterate—that is, \(L(w_t)\)—decreases with \(t\); it should asymptote to \(L^*\). Also, it shows the corresponding curve for random-feature coordinate descent.</p>

</div>


<div class="comments">
<div id="disqus_thread"></div>
<script>
 var disqus_config = function () {
     this.page.url = 'https://galaxie500.github.io/2020/12/29/coordinate-descent.html';
     this.page.identifier = '/2020/12/29/coordinate-descent';
     this.page.title = 'Solving a coordinate descent in 3 ways';
 };

 (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
     var d = document, s = d.createElement('script');

     s.src = '//galaxie500-github-io.disqus.com/embed.js';

     s.setAttribute('data-timestamp', +new Date());
     (d.head || d.body).appendChild(s);
 })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

</div>




<div class="related">
  <h2>related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2019/04/16/impute-columns-pyspark.html">
            Welcome to Jekyll!
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2019/01/09/calibrated_estimation.html">
            Stretching out support vector machine with calibrated estimation
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2019/01/08/airbnb-part-one.html">
            Introducing waiting functionality
          </a>
        </h3>
      </li>
    
  </ul>
</div>




  
  <h2>all tags</h2>
  <div class="tag-cloud"><a href="/tag/introduction/" class="set-5">introduction</a> <a href="/tag/ml/" class="set-3">ml</a> <a href="/tag/nlp/" class="set-2">nlp</a> <a href="/tag/tutorial/" class="set-1">tutorial</a> <a href="/tag/welcome/" class="set-1">welcome</a></div>
  




<script>
  let i = 0;
  const text = 'What is a coordinate descent compared to gradient descent? How to solve it?';
  const speed = parseInt('50');
  
  function typeWriter() {
    if (i < text.length) {
      document.getElementById('animated-post-description').innerHTML += text.charAt(i);
      i++;
      setTimeout(typeWriter, speed);
    }
  }

  document.getElementById('animated-post-description').style.display = 'initial';
  typeWriter();

  // Image modal
  var $imgs = [];
  $('img').each(function(idx) {
    var obj = {
      src: $(this).attr('src')
    }
    $imgs.push(obj);
    var elem = $(this);
    $(this).click(function() {
      $('.modal').magnificPopup('open', idx);
    });
  });

  $('.modal').magnificPopup({
    items: $imgs,
    type: 'image',
    closeOnContentClick: true,
    mainClass: 'mfp-img-mobile',
    image: {
      verticalFit: true
    }
    
  });
</script>

<!--enable latex-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

      </div>
    </main><footer class="site-footer">
  <div class="wrapper">
    <div class="credits"><a href="https://github.com/bitbrain/jekyll-dash">dash</a> theme for Jekyll by <a href="https://github.com/bitbrain">bitbrain</a> made with <i class="fas fa-heart"></i><div class="toggleWrapper">
    <input type="checkbox" class="dn" id="theme-toggle" onclick="modeSwitcher()" checked />
    <label for="theme-toggle" class="toggle">
    <span class="toggle__handler">
      <span class="crater crater--1"></span>
      <span class="crater crater--2"></span>
      <span class="crater crater--3"></span>
    </span>
        <span class="star star--1"></span>
        <span class="star star--2"></span>
        <span class="star star--3"></span>
        <span class="star star--4"></span>
        <span class="star star--5"></span>
        <span class="star star--6"></span>
    </label>
</div>
<script type="text/javascript">
const theme = localStorage.getItem('theme');

if (theme === "light") {
    document.documentElement.setAttribute('data-theme', 'light');
} else {
    document.documentElement.setAttribute('data-theme', 'dark');
}
const userPrefers = getComputedStyle(document.documentElement).getPropertyValue('content');

function activateDarkTheme() {
    document.getElementById('theme-toggle').checked = true;
    document.documentElement.setAttribute('data-theme', 'dark');
    document.documentElement.classList.add('theme--dark');
    document.documentElement.classList.remove('theme--light');
	document.getElementById("theme-toggle").className = 'light';
	window.localStorage.setItem('theme', 'dark');
}

function activateLightTheme() {
    document.getElementById('theme-toggle').checked = false;
    document.documentElement.setAttribute('data-theme', 'light');
    document.documentElement.classList.add('theme--light');
    document.documentElement.classList.remove('theme--dark');
	document.getElementById("theme-toggle").className = 'dark';
	window.localStorage.setItem('theme', 'light');
}

if (theme === "dark") {
    activateDarkTheme();
} else if (theme === "light") {
    activateLightTheme();
} else if  (userPrefers === "light") {
    activateDarkTheme();
} else {
    activateDarkTheme();
}

function modeSwitcher() {
	let currentMode = document.documentElement.getAttribute('data-theme');
	if (currentMode === "dark") {
	    activateLightTheme();
	} else {
	    activateDarkTheme();
	}
}
</script></div>
  </div>
</footer>


<script>
      window.FontAwesomeConfig = {
        searchPseudoElements: true
      }
    </script>

  </body>Í

</html>
